{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6129dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aea0bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_info = {\n",
    "    \"exclude_sentences\": [\n",
    "        \"BASES PARA SUA CONDUTA\",\n",
    "        \"Carlos Bernardo González Pecotche (raumsol)\",\n",
    "    ],\n",
    "    \"001.pdf\": {\n",
    "        \"title\": \"Bases para Sua Conduta\",\n",
    "        \"table_name\": \"ptbr_bases_para_sua_conduta\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 9, 10, 58, 60]\n",
    "    },\n",
    "    \"002.pdf\": {\n",
    "        \"title\": \"A Herança de Si Mesmo\",\n",
    "        \"table_name\": \"ptbr_heranca_si_mesmo\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 10, 29, 30, 31, 32, 33, 34, 35, 36]\n",
    "    },\n",
    "    \"004.pdf\": {\n",
    "        \"title\": \"Deficiências e Propensões do Ser Humano\",\n",
    "        \"table_name\": \"deficiencias_propensoes_ser_humano\",\n",
    "        \"edition\": 13,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 10, 207, 208, 209, 210]\n",
    "    },\n",
    "    \"005.pdf\": {\n",
    "        \"title\": \"Dialógos\",\n",
    "        \"table_name\": \"dialogos\",\n",
    "        \"edition\": 13,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 9, 10, 207, 208, 209, 210, 211, 212, 213, 214, 215]\n",
    "    },\n",
    "    \"006.pdf\": {\n",
    "        \"title\": \"Exegese Logosófica\",\n",
    "        \"table_name\": \"exegese_logosofica\",\n",
    "        \"edition\": 12,\n",
    "        \"exclude_pages\": [3, 4, 5, 6, 8, 9, 10, 11, 12, 110, 111, 112, 113, 114, 115, 116]\n",
    "    },\n",
    "    \"007.pdf\": {\n",
    "        \"title\": \"Intermédio Logosófico\",\n",
    "        \"table_name\": \"intermedio_logosofico\",\n",
    "        \"edition\": 6,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 216, 217, 218, 219, 220]\n",
    "    },\n",
    "    \"008.pdf\": {\n",
    "        \"title\": \"Introdução ao Conhecimento Logosófico\",\n",
    "        \"table_name\": \"introducao_conhecimento_logosofico\",\n",
    "        \"edition\": 4,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 495, 496, 497, 498, 499]\n",
    "    },\n",
    "    \"009.pdf\": {\n",
    "        \"title\": \"Logosofia, Ciência e Método\",\n",
    "        \"table_name\": \"ptbr_logosofia_ciencia_metodo\",\n",
    "        \"edition\": 12,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 9, 10, 11, 12, 148, 149, 150, 151, 152]\n",
    "    },\n",
    "    \"010.pdf\": {\n",
    "        \"title\": \"O espírito\",\n",
    "        \"table_name\": \"o_espirito\",\n",
    "        \"edition\": 5,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 8, 197, 198, 199, 200, 201, 202, 203]\n",
    "    },\n",
    "    \"011.pdf\": {\n",
    "        \"title\": \"O Mecanismo da Vida Consciente\",\n",
    "        \"table_name\": \"mecanismo_vida_consciente\",\n",
    "        \"edition\": 16,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 9, 10, 125, 126, 127, 128, 129, 130, 131]\n",
    "    },\n",
    "    \"012.pdf\": {\n",
    "        \"title\": \"O Senhor de Sándara\",\n",
    "        \"table_name\": \"senhor_sandara\",\n",
    "        \"edition\": 9,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 11, 12, 13, 513, 514, 515]\n",
    "    },\n",
    "    \"013.pdf\": {\n",
    "        \"title\": \"Coletânea da Revista Logosofia - Tomo I\",\n",
    "        \"table_name\": \"tomo_i\",\n",
    "        \"edition\": 2,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 277, 278, 279, 280]\n",
    "    },\n",
    "    \"014.pdf\": {\n",
    "        \"title\": \"Coletânea da Revista Logosofia - Tomo II\",\n",
    "        \"table_name\": \"tomo_ii\",\n",
    "        \"edition\": 3,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 267, 268, 269, 270, 271, 272]\n",
    "    },\n",
    "    \"015.pdf\": {\n",
    "        \"title\": \"Coletânea da Revista Logosofia - Tomo III\",\n",
    "        \"table_name\": \"tomo_iii\",\n",
    "        \"edition\": 3,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 280, 281, 282, 283, 284]\n",
    "    },\n",
    "    \"016.pdf\": {\n",
    "        \"title\": \"Coletânea da Revista Logosofia - Tomo IV\",\n",
    "        \"table_name\": \"tomo_iv\",\n",
    "        \"edition\": 1,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 366, 367, 368, 369, 370, 271, 372]\n",
    "    },\n",
    "    \"017.pdf\": {\n",
    "        \"title\": \"Coletânea da Revista Logosofia - Tomo V\",\n",
    "        \"table_name\": \"tomo_v\",\n",
    "        \"edition\": 1,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 313, 314, 315, 316, 317, 318, 319, 320]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571dc0a",
   "metadata": {},
   "source": [
    "# First step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed01de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def is_endline_next(text: str) -> bool:\n",
    "    \"\"\"Returns if the first character found, excluding empty spaces, is the `\\\\n` token.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the end line is found first, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    for ch in text:\n",
    "        if ch == ' ':\n",
    "            continue\n",
    "\n",
    "        if ch == '\\n':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_ellipsis(text: str) -> bool:\n",
    "    \"\"\"Given a text with lenght 3, if checks if it is a ellipsis `...`.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text, with a dit in the first position.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns True if there is a ellipsis. False otherwise.\n",
    "    \"\"\"    \n",
    "\n",
    "    return text == \"...\"\n",
    "\n",
    "def process_text(text: str, end_line_tokens: list[str] = ['.', '!', '?',]) -> list[str]:\n",
    "    \"\"\"Do the first step of text processing for a given page. This needs to happens before \n",
    "    all other processing. It tries to keep paragraphs organized and\n",
    "    handle some edge cases. It does not remove end of line characters like '\\n'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        end_line_tokens (list[str], optional): The list of tokens that indicate the end of a line. Defaults to ['.', '!', '?']\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The processed list of paragraphs.\n",
    "    \"\"\"    \n",
    "\n",
    "    length = len(text)\n",
    "    buffer = ''\n",
    "    paragraphs = []\n",
    "    ignore_next_new_line = False\n",
    "    skip_next_char = False\n",
    "    i = 0\n",
    "\n",
    "    for ch in text:\n",
    "        # If we reached the end of the text, append the last buffer and return\n",
    "        if i == length - 1:\n",
    "            paragraphs.append(buffer)\n",
    "            return paragraphs\n",
    "        \n",
    "        if skip_next_char:\n",
    "            skip_next_char = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Accounts for that huge graphical letter that starts a paragraph.\n",
    "        # This processing needs to happen before all the others.\n",
    "        if ch == '\\n' and ignore_next_new_line:\n",
    "            ignore_next_new_line = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        buffer += ch\n",
    "\n",
    "        # That must be the huge letter, that starts a new paragraph\n",
    "        if len(buffer) == 1 and buffer.isupper() and is_endline_next(text[i + 1:]):\n",
    "            ignore_next_new_line = True\n",
    "\n",
    "        # Accounts for break of line with '-'\n",
    "        if ch == '-' and is_endline_next(text[i + 1:]):\n",
    "            skip_next_char = True\n",
    "            buffer = buffer[:-1]\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # We're not stopping until we see a real end of line terminator.\n",
    "        if ch in end_line_tokens and is_endline_next(text[i + 1:]):\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "def get_text_and_page(pdf_path: str) -> dict:\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "\n",
    "    book = {}\n",
    "    index = 0\n",
    "    page_count = 1\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "\n",
    "        if text:\n",
    "            current_lines = process_text(text)\n",
    "            book[page_count] = current_lines\n",
    "\n",
    "        page_count += 1\n",
    "        index += 1\n",
    "\n",
    "    return book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae7f68",
   "metadata": {},
   "source": [
    "# Second step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6c5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_short_paragraphs(book: dict, split_by: str = '\\n') -> None:\n",
    "    \"\"\"Deletes paragraphs shorter than a specified length. It removes the\n",
    "    `split_by` characters from the text.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        split_by (str, optional): The delimiter used to split paragraphs. Defaults to '\\n'.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified text with short paragraphs removed.\n",
    "    \"\"\"\n",
    "\n",
    "    empty_pages = []\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        filtered = []\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if split_by not in paragraph and paragraph.strip() != '':\n",
    "                filtered.append(paragraph)\n",
    "                continue\n",
    "\n",
    "            text_splitted = paragraph.split(split_by)\n",
    "            parts = []\n",
    "            for part in text_splitted:\n",
    "                if not part.strip().isdigit():\n",
    "                    parts.extend(part)\n",
    "\n",
    "            joined_parts = ''.join(parts)\n",
    "            if joined_parts.strip() != '':\n",
    "                filtered.extend([joined_parts])\n",
    "\n",
    "        if filtered:\n",
    "            book[page_index] = filtered\n",
    "        else:\n",
    "            empty_pages.append(page_index)\n",
    "\n",
    "    # Remove all keys in the book for pages that are empty\n",
    "    for page_index in empty_pages:\n",
    "        del book[page_index]\n",
    "\n",
    "\n",
    "def find_next_page(book: dict, current_page: int) -> int | None:\n",
    "        \"\"\"Finds the next page in the book.\n",
    "\n",
    "        Args:\n",
    "            book (dict): The book dictionary.\n",
    "            current_page (int): The current page number.\n",
    "\n",
    "        Returns:\n",
    "            int | None: The next page number or None if not found.\n",
    "        \"\"\"\n",
    "        pages_indexes = list(book.keys())\n",
    "        for i, page_index in enumerate(pages_indexes):\n",
    "            if page_index == current_page:\n",
    "                # Return the next page index if it exists\n",
    "                if i + 1 < len(pages_indexes):\n",
    "                    return pages_indexes[i + 1]\n",
    "                break\n",
    "        return None\n",
    "    \n",
    "def concatenate_paragraphs(book: dict, end_line_tokens: list[str] = ['.', '!', '?']) -> None:\n",
    "    \"\"\"If the last page does not finish it's paragraph with ['.', '!', '?'], then it concatenates\n",
    "    the last paragraph of the last page with the first paragraph of the current page. Both lists can\n",
    "    be modified in place. Returns True if so.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book dictionary.\n",
    "        end_line_tokens (list[str]): The list of tokens that defines an end of line.\n",
    "    \"\"\"    \n",
    "\n",
    "    for current_page in book.keys():\n",
    "        try:\n",
    "            last_paragraph = book[current_page][-1].rstrip()\n",
    "        except:\n",
    "            last_paragraph = None\n",
    "\n",
    "        if not last_paragraph or last_paragraph == '':\n",
    "            continue\n",
    "\n",
    "        if not last_paragraph.endswith(tuple(end_line_tokens)):\n",
    "            next_page = find_next_page(book, current_page)\n",
    "            if not next_page:\n",
    "                break\n",
    "            \n",
    "            new_current = last_paragraph + ' ' + book[next_page][0]\n",
    "            new_next = book[next_page][1:]\n",
    "\n",
    "            book[current_page][-1] = new_current\n",
    "            book[next_page] = new_next\n",
    "\n",
    "def eliminate_pages(book: dict, exclude_pages: list[int]) -> None:\n",
    "    \"\"\"Eliminates pages from the book dictionary based on the exclude_pages list.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The dictionary containing page numbers as keys and lists of paragraphs as values.\n",
    "        exclude_pages (list[int]): The list of page numbers to be excluded.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page in exclude_pages:\n",
    "        if page in book:\n",
    "            del book[page]\n",
    "\n",
    "def remove_excluded_sentences(book: dict, excluded_sentences: list[str]) -> None:\n",
    "    \"\"\"Removes a paragraph if it is a prohibited word. It's case sensitive.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        excluded_sentences (list[str]): The prohibited words.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page_count, paragraphs in book.items():\n",
    "        out = []\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip() not in excluded_sentences:\n",
    "                out.append(paragraph)\n",
    "                \n",
    "        book[page_count] = out\n",
    "\n",
    "def post_clean_up(book: dict) -> None:\n",
    "    \"\"\"Removes all end of line breaks and extra whitespaces.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        # If paragraphs is a string, convert to list for uniform processing\n",
    "        if isinstance(paragraphs, str):\n",
    "            paragraphs = [paragraphs]\n",
    "        cleaned_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove end of line breaks and extra whitespaces\n",
    "            cleaned = re.sub(r'\\s+', ' ', paragraph.replace('\\n', ' ')).strip()\n",
    "            if cleaned:\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        book[page_index] = cleaned_paragraphs\n",
    "\n",
    "\n",
    "def delete_hash_symbol(book: dict) -> None:\n",
    "    \"\"\"Removes the title and subtitles, if they are preceed with '#'.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if paragraph[i] == '%':\n",
    "                    if i > 0 and not paragraph[i - 1].isdigit():\n",
    "                        buffer = ''\n",
    "                    elif i > 0 and paragraph[i - 1].isdigit():\n",
    "                        buffer += paragraph[i]\n",
    "                        continue\n",
    "                    elif i == 0:\n",
    "                        buffer = ''\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer != '':\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "        \n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs\n",
    "\n",
    "def delete_titles_subtitles(book: dict) -> None:\n",
    "    \"\"\"Removes the title and subtitles on books.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if i == 0:\n",
    "                    buffer += paragraph[i]\n",
    "                    continue\n",
    "                elif paragraph[i].isupper() and (paragraph[i - 1].islower() or paragraph[i - 1] == ')'):\n",
    "                    buffer = paragraph[i]\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer:\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "\n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs\n",
    "            ''.isupper()\n",
    "\n",
    "def deal_with_quotes(book: dict) -> None:\n",
    "    \"\"\"Separates the quoted paragraphs.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if paragraph[i] == '“' and paragraph[i - 1] == '”':\n",
    "                    cleaned_paragraphs.append(buffer)\n",
    "                    buffer = paragraph[i]\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer:\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "\n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b84424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "book = get_text_and_page(\"books/005.pdf\")\n",
    "eliminate_pages(book, books_info['005.pdf']['exclude_pages'])\n",
    "# delete_hash_symbol(book)\n",
    "delete_short_paragraphs(book)\n",
    "remove_excluded_sentences(book, books_info['exclude_sentences'])\n",
    "# deal_with_quotes(book)\n",
    "concatenate_paragraphs(book)\n",
    "post_clean_up(book)\n",
    "delete_titles_subtitles(book)\n",
    "\n",
    "with open('books/output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(book, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a45a5",
   "metadata": {},
   "source": [
    "# Extracts the cover of .PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a394bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PDF Cover Image Extractor\n",
    "\n",
    "This script extracts cover images from all PDF books in the 'books' folder.\n",
    "It saves the first page of each PDF as a PNG image in a 'covers' folder.\n",
    "\n",
    "Requirements:\n",
    "    pip install PyMuPDF Pillow\n",
    "\n",
    "Usage:\n",
    "    python extract_covers.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def ensure_covers_folder():\n",
    "    \"\"\"Create the covers folder if it doesn't exist.\"\"\"\n",
    "    covers_path = Path(\"covers\")\n",
    "    covers_path.mkdir(exist_ok=True)\n",
    "    return covers_path\n",
    "\n",
    "\n",
    "def extract_cover_image(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract the first page of a PDF as a cover image.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_path (str): Path where the cover image will be saved\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Get the first page\n",
    "        first_page = pdf_document[0]\n",
    "        \n",
    "        # Convert page to image (pixmap)\n",
    "        # Higher matrix values = higher resolution\n",
    "        mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better quality\n",
    "        pix = first_page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert pixmap to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        \n",
    "        # Save as PNG\n",
    "        img.save(output_path, \"PNG\", optimize=True)\n",
    "        \n",
    "        # Close the PDF\n",
    "        pdf_document.close()\n",
    "        \n",
    "        print(f\"✓ Extracted cover for: {Path(pdf_path).name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to extract cover for {Path(pdf_path).name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_pdf_files(books_folder):\n",
    "    \"\"\"\n",
    "    Get all PDF files from the books folder.\n",
    "    \n",
    "    Args:\n",
    "        books_folder (str): Path to the books folder\n",
    "        \n",
    "    Returns:\n",
    "        list: List of PDF file paths\n",
    "    \"\"\"\n",
    "    books_path = Path(books_folder)\n",
    "    \n",
    "    if not books_path.exists():\n",
    "        print(f\"Error: Books folder '{books_folder}' does not exist!\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = list(books_path.glob(\"*.pdf\"))\n",
    "    pdf_files.extend(list(books_path.glob(\"*.PDF\")))  # Case insensitive\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Sanitize filename by removing or replacing problematic characters.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Original filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Sanitized filename\n",
    "    \"\"\"\n",
    "    # Remove extension and replace problematic characters\n",
    "    name = Path(filename).stem\n",
    "    sanitized = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in name)\n",
    "    return sanitized.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all PDF files in the books folder.\"\"\"\n",
    "    books_folder = \"books\"\n",
    "    \n",
    "    print(\"PDF Cover Image Extractor\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Ensure covers folder exists\n",
    "    covers_path = ensure_covers_folder()\n",
    "    print(f\"Cover images will be saved to: {covers_path.absolute()}\")\n",
    "    \n",
    "    # Get all PDF files\n",
    "    pdf_files = get_pdf_files(books_folder)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{books_folder}' folder.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF file(s) to process.\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        # Create output filename\n",
    "        sanitized_name = sanitize_filename(pdf_file.name)\n",
    "        cover_filename = f\"{sanitized_name}_cover.png\"\n",
    "        cover_path = covers_path / cover_filename\n",
    "        \n",
    "        # Extract cover image\n",
    "        if extract_cover_image(str(pdf_file), str(cover_path)):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"✓ Successful: {successful}\")\n",
    "    print(f\"✗ Failed: {failed}\")\n",
    "    print(f\"📁 Cover images saved in: {covers_path.absolute()}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
