{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6129dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\leandro\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU pymupdf pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea0bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_info = {\n",
    "    \"exclude_sentences\": [\n",
    "        \"BASES PARA SUA CONDUTA\",\n",
    "        \"Carlos Bernardo Gonz√°lez Pecotche (raumsol)\",\n",
    "    ],\n",
    "    \"001.pdf\": {\n",
    "        \"title\": \"Bases para Sua Conduta\",\n",
    "        \"table_name\": \"ptbr_bases_para_sua_conduta\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 9, 10, 58, 60]\n",
    "    },\n",
    "    \"002.pdf\": {\n",
    "        \"title\": \"A Heran√ßa de Si Mesmo\",\n",
    "        \"table_name\": \"ptbr_heranca_si_mesmo\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 10, 29, 30, 31, 32, 33, 34, 35, 36]\n",
    "    },\n",
    "    \"004.pdf\": {\n",
    "        \"title\": \"Defici√™ncias e Propens√µes do Ser Humano\",\n",
    "        \"table_name\": \"deficiencias_propensoes_ser_humano\",\n",
    "        \"edition\": 13,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 10, 207, 208, 209, 210]\n",
    "    },\n",
    "    \"005.pdf\": {\n",
    "        \"title\": \"Dial√≥gos\",\n",
    "        \"table_name\": \"dialogos\",\n",
    "        \"edition\": 13,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 9, 10, 207, 208, 209, 210, 211, 212, 213, 214, 215]\n",
    "    },\n",
    "    \"006.pdf\": {\n",
    "        \"title\": \"Exegese Logos√≥fica\",\n",
    "        \"table_name\": \"exegese_logosofica\",\n",
    "        \"edition\": 12,\n",
    "        \"exclude_pages\": [3, 4, 5, 6, 8, 9, 10, 11, 12, 110, 111, 112, 113, 114, 115, 116]\n",
    "    },\n",
    "    \"007.pdf\": {\n",
    "        \"title\": \"Interm√©dio Logos√≥fico\",\n",
    "        \"table_name\": \"intermedio_logosofico\",\n",
    "        \"edition\": 6,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 216, 217, 218, 219, 220]\n",
    "    },\n",
    "    \"008.pdf\": {\n",
    "        \"title\": \"Introdu√ß√£o ao Conhecimento Logos√≥fico\",\n",
    "        \"table_name\": \"introducao_conhecimento_logosofico\",\n",
    "        \"edition\": 4,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 495, 496, 497, 498, 499]\n",
    "    },\n",
    "    \"009.pdf\": {\n",
    "        \"title\": \"Logosofia, Ci√™ncia e M√©todo\",\n",
    "        \"table_name\": \"ptbr_logosofia_ciencia_metodo\",\n",
    "        \"edition\": 12,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 9, 10, 11, 12, 148, 149, 150, 151, 152]\n",
    "    },\n",
    "    \"010.pdf\": {\n",
    "        \"title\": \"O esp√≠rito\",\n",
    "        \"table_name\": \"o_espirito\",\n",
    "        \"edition\": 5,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 8, 197, 198, 199, 200, 201, 202, 203]\n",
    "    },\n",
    "    \"011.pdf\": {\n",
    "        \"title\": \"O Mecanismo da Vida Consciente\",\n",
    "        \"table_name\": \"mecanismo_vida_consciente\",\n",
    "        \"edition\": 16,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 8, 9, 10, 125, 126, 127, 128, 129, 130, 131]\n",
    "    },\n",
    "    \"012.pdf\": {\n",
    "        \"title\": \"O Senhor de S√°ndara\",\n",
    "        \"table_name\": \"senhor_sandara\",\n",
    "        \"edition\": 9,\n",
    "        \"exclude_pages\": [1, 4, 5, 6, 7, 8, 9, 11, 12, 13, 513, 514, 515]\n",
    "    },\n",
    "    \"013.pdf\": {\n",
    "        \"title\": \"Colet√¢nea da Revista Logosofia - Tomo I\",\n",
    "        \"table_name\": \"tomo_i\",\n",
    "        \"edition\": 2,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 277, 278, 279, 280]\n",
    "    },\n",
    "    \"014.pdf\": {\n",
    "        \"title\": \"Colet√¢nea da Revista Logosofia - Tomo II\",\n",
    "        \"table_name\": \"tomo_ii\",\n",
    "        \"edition\": 3,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 267, 268, 269, 270, 271, 272]\n",
    "    },\n",
    "    \"015.pdf\": {\n",
    "        \"title\": \"Colet√¢nea da Revista Logosofia - Tomo III\",\n",
    "        \"table_name\": \"tomo_iii\",\n",
    "        \"edition\": 3,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 280, 281, 282, 283, 284]\n",
    "    },\n",
    "    \"016.pdf\": {\n",
    "        \"title\": \"Colet√¢nea da Revista Logosofia - Tomo IV\",\n",
    "        \"table_name\": \"tomo_iv\",\n",
    "        \"edition\": 1,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 366, 367, 368, 369, 370, 271, 372]\n",
    "    },\n",
    "    \"017.pdf\": {\n",
    "        \"title\": \"Colet√¢nea da Revista Logosofia - Tomo V\",\n",
    "        \"table_name\": \"tomo_v\",\n",
    "        \"edition\": 1,\n",
    "        \"exclude_pages\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 313, 314, 315, 316, 317, 318, 319, 320]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571dc0a",
   "metadata": {},
   "source": [
    "# First step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed01de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def is_endline_next(text: str) -> bool:\n",
    "    \"\"\"Returns if the first character found, excluding empty spaces, is the `\\\\n` token.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the end line is found first, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    for ch in text:\n",
    "        if ch == ' ':\n",
    "            continue\n",
    "\n",
    "        if ch == '\\n':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_ellipsis(text: str) -> bool:\n",
    "    \"\"\"Given a text with lenght 3, if checks if it is a ellipsis `...`.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text, with a dit in the first position.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns True if there is a ellipsis. False otherwise.\n",
    "    \"\"\"    \n",
    "\n",
    "    return text == \"...\"\n",
    "\n",
    "def process_text(text: str, end_line_tokens: list[str] = ['.', '!', '?',]) -> list[str]:\n",
    "    \"\"\"Do the first step of text processing for a given page. This needs to happens before \n",
    "    all other processing. It tries to keep paragraphs organized and\n",
    "    handle some edge cases. It does not remove end of line characters like '\\n'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        end_line_tokens (list[str], optional): The list of tokens that indicate the end of a line. Defaults to ['.', '!', '?']\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The processed list of paragraphs.\n",
    "    \"\"\"    \n",
    "\n",
    "    length = len(text)\n",
    "    buffer = ''\n",
    "    paragraphs = []\n",
    "    ignore_next_new_line = False\n",
    "    skip_next_char = False\n",
    "    i = 0\n",
    "\n",
    "    for ch in text:\n",
    "        # If we reached the end of the text, append the last buffer and return\n",
    "        if i == length - 1:\n",
    "            paragraphs.append(buffer)\n",
    "            return paragraphs\n",
    "        \n",
    "        if skip_next_char:\n",
    "            skip_next_char = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Accounts for that huge graphical letter that starts a paragraph.\n",
    "        # This processing needs to happen before all the others.\n",
    "        if ch == '\\n' and ignore_next_new_line:\n",
    "            ignore_next_new_line = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        buffer += ch\n",
    "\n",
    "        # That must be the huge letter, that starts a new paragraph\n",
    "        if len(buffer) == 1 and buffer.isupper() and is_endline_next(text[i + 1:]):\n",
    "            ignore_next_new_line = True\n",
    "\n",
    "        # Accounts for break of line with '-'\n",
    "        if ch == '-' and is_endline_next(text[i + 1:]):\n",
    "            skip_next_char = True\n",
    "            buffer = buffer[:-1]\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # We're not stopping until we see a real end of line terminator.\n",
    "        if ch in end_line_tokens and is_endline_next(text[i + 1:]):\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "def get_text_and_page(pdf_path: str) -> dict:\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "\n",
    "    book = {}\n",
    "    index = 0\n",
    "    page_count = 1\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "\n",
    "        if text:\n",
    "            current_lines = process_text(text)\n",
    "            book[page_count] = current_lines\n",
    "\n",
    "        page_count += 1\n",
    "        index += 1\n",
    "\n",
    "    return book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae7f68",
   "metadata": {},
   "source": [
    "# Second step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc6c5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_short_paragraphs(book: dict, split_by: str = '\\n') -> None:\n",
    "    \"\"\"Deletes paragraphs shorter than a specified length. It removes the\n",
    "    `split_by` characters from the text.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        split_by (str, optional): The delimiter used to split paragraphs. Defaults to '\\n'.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified text with short paragraphs removed.\n",
    "    \"\"\"\n",
    "\n",
    "    empty_pages = []\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        filtered = []\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if split_by not in paragraph and paragraph.strip() != '':\n",
    "                filtered.append(paragraph)\n",
    "                continue\n",
    "\n",
    "            text_splitted = paragraph.split(split_by)\n",
    "            parts = []\n",
    "            for part in text_splitted:\n",
    "                if not part.strip().isdigit():\n",
    "                    parts.extend(part)\n",
    "\n",
    "            joined_parts = ''.join(parts)\n",
    "            if joined_parts.strip() != '':\n",
    "                filtered.extend([joined_parts])\n",
    "\n",
    "        if filtered:\n",
    "            book[page_index] = filtered\n",
    "        else:\n",
    "            empty_pages.append(page_index)\n",
    "\n",
    "    # Remove all keys in the book for pages that are empty\n",
    "    for page_index in empty_pages:\n",
    "        del book[page_index]\n",
    "\n",
    "\n",
    "def find_next_page(book: dict, current_page: int) -> int | None:\n",
    "        \"\"\"Finds the next page in the book.\n",
    "\n",
    "        Args:\n",
    "            book (dict): The book dictionary.\n",
    "            current_page (int): The current page number.\n",
    "\n",
    "        Returns:\n",
    "            int | None: The next page number or None if not found.\n",
    "        \"\"\"\n",
    "        pages_indexes = list(book.keys())\n",
    "        for i, page_index in enumerate(pages_indexes):\n",
    "            if page_index == current_page:\n",
    "                # Return the next page index if it exists\n",
    "                if i + 1 < len(pages_indexes):\n",
    "                    return pages_indexes[i + 1]\n",
    "                break\n",
    "        return None\n",
    "    \n",
    "def concatenate_paragraphs(book: dict, end_line_tokens: list[str] = ['.', '!', '?']) -> None:\n",
    "    \"\"\"If the last page does not finish it's paragraph with ['.', '!', '?'], then it concatenates\n",
    "    the last paragraph of the last page with the first paragraph of the current page. Both lists can\n",
    "    be modified in place.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book dictionary.\n",
    "        end_line_tokens (list[str]): The list of tokens that defines an end of line.\n",
    "    \"\"\"    \n",
    "\n",
    "    for current_page in book.keys():\n",
    "        try:\n",
    "            last_paragraph = book[current_page][-1].rstrip()\n",
    "        except:\n",
    "            last_paragraph = None\n",
    "\n",
    "        if not last_paragraph or last_paragraph == '':\n",
    "            continue\n",
    "\n",
    "        if not last_paragraph.endswith(tuple(end_line_tokens)):\n",
    "            next_page = find_next_page(book, current_page)\n",
    "            if not next_page:\n",
    "                break\n",
    "            \n",
    "            new_current = last_paragraph + ' ' + book[next_page][0]\n",
    "            new_next = book[next_page][1:]\n",
    "\n",
    "            book[current_page][-1] = new_current\n",
    "            book[next_page] = new_next\n",
    "\n",
    "def eliminate_pages(book: dict, exclude_pages: list[int]) -> None:\n",
    "    \"\"\"Eliminates pages from the book dictionary based on the exclude_pages list.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The dictionary containing page numbers as keys and lists of paragraphs as values.\n",
    "        exclude_pages (list[int]): The list of page numbers to be excluded.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page in exclude_pages:\n",
    "        if page in book:\n",
    "            del book[page]\n",
    "\n",
    "def remove_excluded_sentences(book: dict, excluded_sentences: list[str]) -> None:\n",
    "    \"\"\"Removes a paragraph if it is a prohibited word. It's case sensitive.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        excluded_sentences (list[str]): The prohibited words.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page_count, paragraphs in book.items():\n",
    "        out = []\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip() not in excluded_sentences:\n",
    "                out.append(paragraph)\n",
    "                \n",
    "        book[page_count] = out\n",
    "\n",
    "def post_clean_up(book: dict) -> None:\n",
    "    \"\"\"Removes all end of line breaks and extra whitespaces.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        # If paragraphs is a string, convert to list for uniform processing\n",
    "        if isinstance(paragraphs, str):\n",
    "            paragraphs = [paragraphs]\n",
    "        cleaned_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove end of line breaks and extra whitespaces\n",
    "            cleaned = re.sub(r'\\s+', ' ', paragraph.replace('\\n', ' ')).strip()\n",
    "            if cleaned:\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        book[page_index] = cleaned_paragraphs\n",
    "\n",
    "\n",
    "def delete_hash_symbol(book: dict) -> None:\n",
    "    \"\"\"Removes the title and subtitles, if they are preceed with '#'.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if paragraph[i] == '%':\n",
    "                    if i > 0 and not paragraph[i - 1].isdigit():\n",
    "                        buffer = ''\n",
    "                    elif i > 0 and paragraph[i - 1].isdigit():\n",
    "                        buffer += paragraph[i]\n",
    "                        continue\n",
    "                    elif i == 0:\n",
    "                        buffer = ''\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer != '':\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "        \n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs\n",
    "\n",
    "def delete_titles_subtitles(book: dict) -> None:\n",
    "    \"\"\"Removes the title and subtitles on books.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if i == 0:\n",
    "                    buffer += paragraph[i]\n",
    "                    continue\n",
    "                elif paragraph[i].isupper() and (paragraph[i - 1].islower() or paragraph[i - 1] == ')'):\n",
    "                    buffer = paragraph[i]\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer:\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "\n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs\n",
    "            ''.isupper()\n",
    "\n",
    "def deal_with_quotes(book: dict) -> None:\n",
    "    \"\"\"Separates the quoted paragraphs.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page, paragraphs in book.items():\n",
    "        cleaned_paragraphs = []\n",
    "\n",
    "        # Deal with symbols that ais translated to '#'.\n",
    "        for paragraph in paragraphs:\n",
    "            buffer = ''\n",
    "            for i in range(0, len(paragraph)):\n",
    "                if paragraph[i] == '‚Äú' and paragraph[i - 1] == '‚Äù':\n",
    "                    cleaned_paragraphs.append(buffer)\n",
    "                    buffer = paragraph[i]\n",
    "                else:\n",
    "                    buffer += paragraph[i]\n",
    "\n",
    "            if buffer:\n",
    "                cleaned_paragraphs.append(buffer)\n",
    "\n",
    "        if cleaned_paragraphs:\n",
    "            book[page] = cleaned_paragraphs\n",
    "\n",
    "def concatenate_dialogs(book: dict) -> None:\n",
    "    \"\"\"A dialog is a sentence that starts with a '‚Äî'. Tries to have three dialogs into one line. \n",
    "    If the next paragraph also starts with '‚Äî', it gets appedend by a '\\n'. If a paragraph already \n",
    "    contains three dialogs or the next paragraph does not starts with '‚Äî', the cicle is broken.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\" \n",
    "\n",
    "    # 1. \"Aplanar\" o livro: Criar uma lista sequencial com (numero_pagina, texto)\n",
    "    # Isso nos permite olhar para o \"pr√≥ximo\" par√°grafo sem se preocupar com a virada de p√°gina manual.\n",
    "    stream = []\n",
    "    for page, paragraphs in book.items():\n",
    "        for paragraph in paragraphs:\n",
    "            stream.append((page, paragraph))\n",
    "\n",
    "    # Dicion√°rio tempor√°rio para guardar o novo conte√∫do organizado\n",
    "    new_book_content = {page: [] for page in book}\n",
    "\n",
    "    dialog_buffer = []       # Armazena os di√°logos atuais (m√°x 3)\n",
    "    current_group_page = None # A p√°gina onde o grupo atual de di√°logos come√ßou\n",
    "\n",
    "    for page, text in stream:\n",
    "        # Verifica se √© um di√°logo (remove espa√ßos em branco extras antes de checar)\n",
    "        is_dialog = text.strip().startswith('‚Äî')\n",
    "\n",
    "        if is_dialog:\n",
    "            # Se o buffer est√° vazio, este √© o primeiro di√°logo do grupo.\n",
    "            # Marcamos a p√°gina atual como a \"dona\" deste grupo.\n",
    "            if not dialog_buffer:\n",
    "                current_group_page = page\n",
    "            \n",
    "            dialog_buffer.append(text)\n",
    "\n",
    "            # Se atingiu o limite de 3, fecha o grupo\n",
    "            if len(dialog_buffer) == 3:\n",
    "                merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "                new_book_content[current_group_page].append(merged_dialog)\n",
    "                dialog_buffer = [] # Limpa o buffer\n",
    "                current_group_page = None\n",
    "\n",
    "        else:\n",
    "            # Se encontramos um texto que N√ÉO √© di√°logo, o ciclo de agrupamento quebra.\n",
    "            \n",
    "            # Primeiro: Se havia algo no buffer (1 ou 2 di√°logos), salvamos onde est√£o.\n",
    "            if dialog_buffer:\n",
    "                merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "                new_book_content[current_group_page].append(merged_dialog)\n",
    "                dialog_buffer = []\n",
    "                current_group_page = None\n",
    "            \n",
    "            # Segundo: Adicionamos o par√°grafo atual (n√£o-di√°logo) na sua p√°gina original\n",
    "            new_book_content[page].append(text)\n",
    "\n",
    "    # Caso especial: O livro acabou, mas ainda sobrou algo no buffer\n",
    "    if dialog_buffer:\n",
    "        merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "        new_book_content[current_group_page].append(merged_dialog)\n",
    "\n",
    "    # 3. Atualizar o dicion√°rio original (in-place)\n",
    "    # Limpamos as listas originais e inserimos as novas listas processadas\n",
    "    for page in book:\n",
    "        book[page].clear()\n",
    "        book[page].extend(new_book_content.get(page, []))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b84424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "pdf_filename = \"005.pdf\"\n",
    "book = get_text_and_page(f\"books/{pdf_filename}\")\n",
    "eliminate_pages(book, books_info[pdf_filename]['exclude_pages'])\n",
    "# delete_hash_symbol(book)\n",
    "delete_short_paragraphs(book)\n",
    "remove_excluded_sentences(book, books_info['exclude_sentences'])\n",
    "# deal_with_quotes(book)\n",
    "concatenate_paragraphs(book)\n",
    "post_clean_up(book)\n",
    "delete_titles_subtitles(book)\n",
    "concatenate_dialogs(book)\n",
    "\n",
    "with open('books/output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(book, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a474cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_xlxs(pdf_filename: str) -> None:\n",
    "    with open('books/output.json', 'r', encoding='utf-8') as f:\n",
    "        book = json.loads(f.read())\n",
    "\n",
    "    # 1. Encontrar o n√∫mero m√°ximo de par√°grafos em uma √∫nica p√°gina para nivelar a tabela\n",
    "    max_len = max((len(paras) for paras in book.values()), default=0)\n",
    "\n",
    "    # 2. Normalizar os dados (preencher listas menores com None para igualar o tamanho)\n",
    "    data_for_excel = {\n",
    "        page: paras + [None] * (max_len - len(paras))\n",
    "        for page, paras in book.items()\n",
    "    }\n",
    "\n",
    "    # 3. Criar DataFrame e salvar\n",
    "    df = pd.DataFrame(data_for_excel)\n",
    "\n",
    "    # Se as chaves das p√°ginas forem n√∫meros, ordenamos as colunas\n",
    "    # df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "    output_filename = pdf_filename.replace('.pdf', '.xlsx')\n",
    "    df.to_excel(f\"books/{output_filename}\", index=False)\n",
    "\n",
    "json_to_xlxs(pdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a45a5",
   "metadata": {},
   "source": [
    "# Extracts the cover of .PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a394bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PDF Cover Image Extractor\n",
    "\n",
    "This script extracts cover images from all PDF books in the 'books' folder.\n",
    "It saves the first page of each PDF as a PNG image in a 'covers' folder.\n",
    "\n",
    "Requirements:\n",
    "    pip install PyMuPDF Pillow\n",
    "\n",
    "Usage:\n",
    "    python extract_covers.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def ensure_covers_folder():\n",
    "    \"\"\"Create the covers folder if it doesn't exist.\"\"\"\n",
    "    covers_path = Path(\"covers\")\n",
    "    covers_path.mkdir(exist_ok=True)\n",
    "    return covers_path\n",
    "\n",
    "\n",
    "def extract_cover_image(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract the first page of a PDF as a cover image.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_path (str): Path where the cover image will be saved\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Get the first page\n",
    "        first_page = pdf_document[0]\n",
    "        \n",
    "        # Convert page to image (pixmap)\n",
    "        # Higher matrix values = higher resolution\n",
    "        mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better quality\n",
    "        pix = first_page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert pixmap to PIL Image\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        \n",
    "        # Save as PNG\n",
    "        img.save(output_path, \"PNG\", optimize=True)\n",
    "        \n",
    "        # Close the PDF\n",
    "        pdf_document.close()\n",
    "        \n",
    "        print(f\"‚úì Extracted cover for: {Path(pdf_path).name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to extract cover for {Path(pdf_path).name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_pdf_files(books_folder):\n",
    "    \"\"\"\n",
    "    Get all PDF files from the books folder.\n",
    "    \n",
    "    Args:\n",
    "        books_folder (str): Path to the books folder\n",
    "        \n",
    "    Returns:\n",
    "        list: List of PDF file paths\n",
    "    \"\"\"\n",
    "    books_path = Path(books_folder)\n",
    "    \n",
    "    if not books_path.exists():\n",
    "        print(f\"Error: Books folder '{books_folder}' does not exist!\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = list(books_path.glob(\"*.pdf\"))\n",
    "    pdf_files.extend(list(books_path.glob(\"*.PDF\")))  # Case insensitive\n",
    "    \n",
    "    return pdf_files\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Sanitize filename by removing or replacing problematic characters.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Original filename\n",
    "        \n",
    "    Returns:\n",
    "        str: Sanitized filename\n",
    "    \"\"\"\n",
    "    # Remove extension and replace problematic characters\n",
    "    name = Path(filename).stem\n",
    "    sanitized = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in name)\n",
    "    return sanitized.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process all PDF files in the books folder.\"\"\"\n",
    "    books_folder = \"books\"\n",
    "    \n",
    "    print(\"PDF Cover Image Extractor\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Ensure covers folder exists\n",
    "    covers_path = ensure_covers_folder()\n",
    "    print(f\"Cover images will be saved to: {covers_path.absolute()}\")\n",
    "    \n",
    "    # Get all PDF files\n",
    "    pdf_files = get_pdf_files(books_folder)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{books_folder}' folder.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF file(s) to process.\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        # Create output filename\n",
    "        sanitized_name = sanitize_filename(pdf_file.name)\n",
    "        cover_filename = f\"{sanitized_name}_cover.png\"\n",
    "        cover_path = covers_path / cover_filename\n",
    "        \n",
    "        # Extract cover image\n",
    "        if extract_cover_image(str(pdf_file), str(cover_path)):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"‚úì Successful: {successful}\")\n",
    "    print(f\"‚úó Failed: {failed}\")\n",
    "    print(f\"üìÅ Cover images saved in: {covers_path.absolute()}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56290c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Antes ---\n",
      "{\n",
      "  \"1\": [\n",
      "    \"Era uma vez em uma terra distante.\",\n",
      "    \"- Ol√°, viajante.\",\n",
      "    \"- Ol√°, quem √© voc√™?\"\n",
      "  ],\n",
      "  \"2\": [\n",
      "    \"- Sou o guardi√£o.\",\n",
      "    \"O viajante ficou espantado.\",\n",
      "    \"- Posso passar?\",\n",
      "    \"- N√£o pode.\",\n",
      "    \"- Por favor.\",\n",
      "    \"- Est√° bem, passe.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- Depois ---\n",
      "{\n",
      "  \"1\": [\n",
      "    \"Era uma vez em uma terra distante.\",\n",
      "    \"- Ol√°, viajante.\\n- Ol√°, quem √© voc√™?\\n- Sou o guardi√£o.\"\n",
      "  ],\n",
      "  \"2\": [\n",
      "    \"O viajante ficou espantado.\",\n",
      "    \"- Posso passar?\\n- N√£o pode.\\n- Por favor.\",\n",
      "    \"- Est√° bem, passe.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def concatenate_dialogs(book: dict) -> None:\n",
    "    \"\"\"\n",
    "    Agrupa di√°logos (par√°grafos iniciando com '-') em blocos de no m√°ximo 3.\n",
    "    Se um grupo de di√°logos come√ßar em uma p√°gina e terminar na outra,\n",
    "    o grupo completo √© movido para a p√°gina onde come√ßou.\n",
    "    \n",
    "    A modifica√ß√£o √© feita in-place (diretamente no objeto 'book').\n",
    "\n",
    "    Args:\n",
    "        book (dict): O livro onde chaves s√£o p√°ginas e valores s√£o listas de par√°grafos.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. \"Aplanar\" o livro: Criar uma lista sequencial com (numero_pagina, texto)\n",
    "    # Isso nos permite olhar para o \"pr√≥ximo\" par√°grafo sem se preocupar com a virada de p√°gina manual.\n",
    "    stream = []\n",
    "    for page, paragraphs in book.items():\n",
    "        for paragraph in paragraphs:\n",
    "            stream.append((page, paragraph))\n",
    "\n",
    "    # Dicion√°rio tempor√°rio para guardar o novo conte√∫do organizado\n",
    "    new_book_content = {page: [] for page in book}\n",
    "\n",
    "    dialog_buffer = []       # Armazena os di√°logos atuais (m√°x 3)\n",
    "    current_group_page = None # A p√°gina onde o grupo atual de di√°logos come√ßou\n",
    "\n",
    "    for page, text in stream:\n",
    "        # Verifica se √© um di√°logo (remove espa√ßos em branco extras antes de checar)\n",
    "        is_dialog = text.strip().startswith('-')\n",
    "\n",
    "        if is_dialog:\n",
    "            # Se o buffer est√° vazio, este √© o primeiro di√°logo do grupo.\n",
    "            # Marcamos a p√°gina atual como a \"dona\" deste grupo.\n",
    "            if not dialog_buffer:\n",
    "                current_group_page = page\n",
    "            \n",
    "            dialog_buffer.append(text)\n",
    "\n",
    "            # Se atingiu o limite de 3, fecha o grupo\n",
    "            if len(dialog_buffer) == 3:\n",
    "                merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "                new_book_content[current_group_page].append(merged_dialog)\n",
    "                dialog_buffer = [] # Limpa o buffer\n",
    "                current_group_page = None\n",
    "\n",
    "        else:\n",
    "            # Se encontramos um texto que N√ÉO √© di√°logo, o ciclo de agrupamento quebra.\n",
    "            \n",
    "            # Primeiro: Se havia algo no buffer (1 ou 2 di√°logos), salvamos onde est√£o.\n",
    "            if dialog_buffer:\n",
    "                merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "                new_book_content[current_group_page].append(merged_dialog)\n",
    "                dialog_buffer = []\n",
    "                current_group_page = None\n",
    "            \n",
    "            # Segundo: Adicionamos o par√°grafo atual (n√£o-di√°logo) na sua p√°gina original\n",
    "            new_book_content[page].append(text)\n",
    "\n",
    "    # Caso especial: O livro acabou, mas ainda sobrou algo no buffer\n",
    "    if dialog_buffer:\n",
    "        merged_dialog = \"\\n\".join(dialog_buffer)\n",
    "        new_book_content[current_group_page].append(merged_dialog)\n",
    "\n",
    "    # 3. Atualizar o dicion√°rio original (in-place)\n",
    "    # Limpamos as listas originais e inserimos as novas listas processadas\n",
    "    for page in book:\n",
    "        book[page].clear()\n",
    "        book[page].extend(new_book_content.get(page, []))\n",
    "\n",
    "# --- Exemplo de Uso ---\n",
    "\n",
    "livro_exemplo = {\n",
    "    \"1\": [\n",
    "        \"Era uma vez em uma terra distante.\",\n",
    "        \"- Ol√°, viajante.\",\n",
    "        \"- Ol√°, quem √© voc√™?\"\n",
    "    ],\n",
    "    \"2\": [\n",
    "        \"- Sou o guardi√£o.\",\n",
    "        \"O viajante ficou espantado.\",\n",
    "        \"- Posso passar?\",\n",
    "        \"- N√£o pode.\",\n",
    "        \"- Por favor.\",\n",
    "        \"- Est√° bem, passe.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"--- Antes ---\")\n",
    "import json\n",
    "print(json.dumps(livro_exemplo, indent=2, ensure_ascii=False))\n",
    "\n",
    "concatenate_dialogs(livro_exemplo)\n",
    "\n",
    "print(\"\\n--- Depois ---\")\n",
    "print(json.dumps(livro_exemplo, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
