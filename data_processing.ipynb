{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6129dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -qU pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_info = {\n",
    "    \"exclude_sentences\": [\n",
    "        'BASES PARA SUA CONDUTA',\n",
    "    ],\n",
    "    \"001.pdf\": {\n",
    "        \"title\": \"Bases para Sua Conduta\",\n",
    "        \"table_name\": \"ptbr_bases_para_sua_conduta\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 7, 9, 10, 58, 60]\n",
    "    },\n",
    "    \"002.pdf\": {\n",
    "        \"title\": \"A Herança de Si Mesmo\",\n",
    "        \"table_name\": \"ptbr_heranca_si_mesmo\",\n",
    "        \"edition\": 22,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 8, 10, 29, 30, 31, 32, 33, 34, 35, 36]\n",
    "    },\n",
    "    \"009.pdf\": {\n",
    "        \"title\": \"Logosofia, Ciência e Método\",\n",
    "        \"table_name\": \"ptbr_logosofia_ciencia_metodo\",\n",
    "        \"edition\": 12,\n",
    "        \"exclude_pages\": [1, 3, 4, 5, 6, 9, 10, 11, 12, 148, 149, 150, 151, 152]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571dc0a",
   "metadata": {},
   "source": [
    "# First step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed01de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def is_endline_next(text: str) -> bool:\n",
    "    \"\"\"Returns if the first character found, excluding empty spaces, is the `\\\\n` token.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the end line is found first, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    for ch in text:\n",
    "        if ch == ' ':\n",
    "            continue\n",
    "\n",
    "        if ch == '\\n':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_ellipsis(text: str) -> bool:\n",
    "    \"\"\"Given a text with lenght 3, if checks if it is a ellipsis `...`.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text, with a dit in the first position.\n",
    "\n",
    "    Returns:\n",
    "        bool: Returns True if there is a ellipsis. False otherwise.\n",
    "    \"\"\"    \n",
    "\n",
    "    return text == \"...\"\n",
    "\n",
    "def process_text(text: str, end_line_tokens: list[str] = ['.', '!', '?',]) -> list[str]:\n",
    "    \"\"\"Do the first step of text processing for a given page. This needs to happens before \n",
    "    all other processing. It tries to keep paragraphs organized and\n",
    "    handle some edge cases. It does not remove end of line characters like '\\n'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        end_line_tokens (list[str], optional): The list of tokens that indicate the end of a line. Defaults to ['.', '!', '?']\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The processed list of paragraphs.\n",
    "    \"\"\"    \n",
    "\n",
    "    length = len(text)\n",
    "    buffer = ''\n",
    "    paragraphs = []\n",
    "    ignore_next_new_line = False\n",
    "    skip_next_char = False\n",
    "    i = 0\n",
    "\n",
    "    for ch in text:\n",
    "        # If we reached the end of the text, append the last buffer and return\n",
    "        if i == length - 1:\n",
    "            paragraphs.append(buffer)\n",
    "            return paragraphs\n",
    "        \n",
    "        if skip_next_char:\n",
    "            skip_next_char = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Accounts for that huge graphical letter that starts a paragraph.\n",
    "        # This processing needs to happen before all the others.\n",
    "        if ch == '\\n' and ignore_next_new_line:\n",
    "            ignore_next_new_line = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        buffer += ch\n",
    "\n",
    "        # That must be the huge letter, that starts a new paragraph\n",
    "        if len(buffer) == 1 and buffer.isupper() and is_endline_next(text[i + 1:]):\n",
    "            ignore_next_new_line = True\n",
    "\n",
    "        # Accounts for break of line with '-'\n",
    "        if ch == '-' and is_endline_next(text[i + 1:]):\n",
    "            skip_next_char = True\n",
    "            buffer = buffer[:-1]\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # We're not stopping until we see a real end of line terminator.\n",
    "        if ch in end_line_tokens and is_endline_next(text[i + 1:]):\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "def get_text_and_page(pdf_path: str) -> dict:\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "\n",
    "    book = {}\n",
    "    index = 0\n",
    "    page_count = 1\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text()\n",
    "\n",
    "        if text:\n",
    "            current_lines = process_text(text)\n",
    "            book[page_count] = current_lines\n",
    "\n",
    "        page_count += 1\n",
    "        index += 1\n",
    "\n",
    "    return book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae7f68",
   "metadata": {},
   "source": [
    "# Second step of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6c5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_short_paragraphs(book: dict, split_by: str = '\\n') -> None:\n",
    "    \"\"\"Deletes paragraphs shorter than a specified length. It removes the\n",
    "    `split_by` characters from the text.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        split_by (str, optional): The delimiter used to split paragraphs. Defaults to '\\n'.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified text with short paragraphs removed.\n",
    "    \"\"\"\n",
    "\n",
    "    empty_pages = []\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        filtered = []\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            if split_by not in paragraph and paragraph.strip() != '':\n",
    "                filtered.append(paragraph)\n",
    "                continue\n",
    "\n",
    "            text_splitted = paragraph.split(split_by)\n",
    "            parts = []\n",
    "            for part in text_splitted:\n",
    "                if not part.strip().isdigit():\n",
    "                    parts.extend(part)\n",
    "\n",
    "            joined_parts = ''.join(parts)\n",
    "            if joined_parts.strip() != '':\n",
    "                filtered.extend([joined_parts])\n",
    "\n",
    "        if filtered:\n",
    "            book[page_index] = filtered\n",
    "        else:\n",
    "            empty_pages.append(page_index)\n",
    "\n",
    "    # Remove all keys in the book for pages that are empty\n",
    "    for page_index in empty_pages:\n",
    "        del book[page_index]\n",
    "\n",
    "def find_next_page(book: dict, current_page: int) -> int | None:\n",
    "        \"\"\"Finds the next page in the book.\n",
    "\n",
    "        Args:\n",
    "            book (dict): The book dictionary.\n",
    "            current_page (int): The current page number.\n",
    "\n",
    "        Returns:\n",
    "            int | None: The next page number or None if not found.\n",
    "        \"\"\"\n",
    "        pages_indexes = list(book.keys())\n",
    "        for i, page_index in enumerate(pages_indexes):\n",
    "            if page_index == current_page:\n",
    "                # Return the next page index if it exists\n",
    "                if i + 1 < len(pages_indexes):\n",
    "                    return pages_indexes[i + 1]\n",
    "                break\n",
    "        return None\n",
    "    \n",
    "def concatenate_paragraphs(book: dict, end_line_tokens: list[str] = ['.', '!', '?']) -> None:\n",
    "    \"\"\"If the last page does not finish it's paragraph with ['.', '!', '?'], then it concatenates\n",
    "    the last paragraph of the last page with the first paragraph of the current page. Both lists can\n",
    "    be modified in place. Returns True if so.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book dictionary.\n",
    "        end_line_tokens (list[str]): The list of tokens that defines an end of line.\n",
    "    \"\"\"    \n",
    "\n",
    "    for current_page in book.keys():\n",
    "        try:\n",
    "            last_paragraph = book[current_page][-1].rstrip()\n",
    "        except:\n",
    "            last_paragraph = None\n",
    "\n",
    "        if not last_paragraph or last_paragraph == '':\n",
    "            continue\n",
    "\n",
    "        if not last_paragraph.endswith(tuple(end_line_tokens)):\n",
    "            next_page = find_next_page(book, current_page)\n",
    "            if not next_page:\n",
    "                break\n",
    "            \n",
    "            new_current = last_paragraph + ' ' + book[next_page][0]\n",
    "            new_next = book[next_page][1:]\n",
    "\n",
    "            book[current_page][-1] = new_current\n",
    "            book[next_page] = new_next\n",
    "\n",
    "def eliminate_pages(book: dict, exclude_pages: list[int]) -> None:\n",
    "    \"\"\"Eliminates pages from the book dictionary based on the exclude_pages list.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The dictionary containing page numbers as keys and lists of paragraphs as values.\n",
    "        exclude_pages (list[int]): The list of page numbers to be excluded.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page in exclude_pages:\n",
    "        if page in book:\n",
    "            del book[page]\n",
    "\n",
    "def remove_excluded_sentences(book: dict, excluded_sentences: list[str]) -> None:\n",
    "    \"\"\"Removes a paragraph if it is a prohibited word. It's case sensitive.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "        excluded_sentences (list[str]): The prohibited words.\n",
    "    \"\"\"    \n",
    "\n",
    "    for page_count, paragraphs in book.items():\n",
    "        out = []\n",
    "        for paragraph in paragraphs:\n",
    "            if paragraph.strip() not in excluded_sentences:\n",
    "                out.append(paragraph)\n",
    "                \n",
    "        book[page_count] = out\n",
    "\n",
    "def post_clean_up(book: dict) -> None:\n",
    "    \"\"\"Removes all end of line breaks and extra whitespaces.\n",
    "\n",
    "    Args:\n",
    "        book (dict): The book.\n",
    "    \"\"\"\n",
    "\n",
    "    for page_index, paragraphs in book.items():\n",
    "        # If paragraphs is a string, convert to list for uniform processing\n",
    "        if isinstance(paragraphs, str):\n",
    "            paragraphs = [paragraphs]\n",
    "        cleaned_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # Remove end of line breaks and extra whitespaces\n",
    "            cleaned = re.sub(r'\\s+', ' ', paragraph.replace('\\n', ' ')).strip()\n",
    "            if cleaned:\n",
    "                cleaned_paragraphs.append(cleaned)\n",
    "        book[page_index] = cleaned_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b84424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "book = get_text_and_page(\"books/002.pdf\")\n",
    "eliminate_pages(book, books_info['002.pdf']['exclude_pages'])\n",
    "delete_short_paragraphs(book)\n",
    "remove_excluded_sentences(book, books_info['exclude_sentences'])\n",
    "concatenate_paragraphs(book)\n",
    "post_clean_up(book)\n",
    "\n",
    "with open('output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(book, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
